{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9352b8",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "### 1.1 Loading Datasets\n",
    "\n",
    "**Document Type** = Article, Article in Press, Review, Book Chapter, Letter, Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334712e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "\n",
    "from itertools import chain, combinations\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from unidecode import unidecode\n",
    "from collections import Counter\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f3c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorsPath = \"data/authors.xlsx\"\n",
    "epidemiologyPath = \"data/epidemiology.xlsx\"\n",
    "immunologyPath = \"data/immunology.xlsx\"\n",
    "infectiveDiseasesPath = \"data/infectious_diseases.xlsx\"\n",
    "microbiologyPath = \"data/microbiology.xlsx\"\n",
    "\n",
    "authorsData = pd.read_excel(authorsPath)\n",
    "\n",
    "epidemiologyData = pd.read_excel(epidemiologyPath)\n",
    "immunologyData = pd.read_excel(immunologyPath)\n",
    "infectiveDiseasesData = pd.read_excel(infectiveDiseasesPath)\n",
    "microbiologyData = pd.read_excel(microbiologyPath)\n",
    "\n",
    "documentTypeSet = {\"Article\", \"Article in Press\", \"Review\", \"Book Chapter\", \"Letter\", \"Note\"}\n",
    "\n",
    "duplicatesSubset = ['Authors', 'Title', 'Year', 'Source title', 'Document Type']\n",
    "\n",
    "print(f\"Autori - Rows: {authorsData.shape[0]} - Cols: {authorsData.shape[1]}\")\n",
    "print(f\"Epidemiology - Rows: {epidemiologyData.shape[0]} - Cols: {epidemiologyData.shape[1]}\")\n",
    "print(f\"Immunology - Rows: {immunologyData.shape[0]} - Cols: {immunologyData.shape[1]}\")\n",
    "print(f\"Infectious diseases - Rows: {infectiveDiseasesData.shape[0]} - Cols: {infectiveDiseasesData.shape[1]}\")\n",
    "print(f\"Microbiology - Rows: {microbiologyData.shape[0]} - Cols: {microbiologyData.shape[1]}\")\n",
    "\n",
    "# Global helper functions\n",
    "\n",
    "def convertSerbianCharsToAscii(df, columns):\n",
    "    serbianLatinCharsMap = {\n",
    "        'ć': 'c',\n",
    "        'č': 'c',\n",
    "        'đ': 'dj',\n",
    "        'š': 's',\n",
    "        'ž': 'z',\n",
    "        \n",
    "        'Ć': 'C',\n",
    "        'Č': 'C',\n",
    "        'Đ': 'Dj',\n",
    "        'Š': 'S',\n",
    "        'Ž': 'Z',\n",
    "    }\n",
    "    \n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: unidecode(x,  errors='strict'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdffec8f",
   "metadata": {},
   "source": [
    "### 1.2 Dataset Preprocessing with Authors\n",
    "Some authors were identified whose **Number of Publications** and **H-index** are null or unknown.  \n",
    "\n",
    "**Number of Publications** – will be set to 0 for now, and we will attempt to fill it in later from the corresponding department dataset.  \n",
    "**H-index** – will be set to 0.  \n",
    "**Short Name** – a new column has been added representing the author's short name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac5fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorsData.fillna(0, inplace=True)\n",
    "authorsData['H index'] = authorsData['H index'].replace('?', 0)\n",
    "authorsData = authorsData.astype({'H index': int, 'Number of rows': int})\n",
    "\n",
    "convertSerbianCharsToAscii(authorsData, ['Name', 'Surname', 'Department'])\n",
    "\n",
    "authorsData.insert(0,'id_author',0)\n",
    "authorsData['id_author'] = authorsData.reset_index().index + 1\n",
    "\n",
    "authorsData['Name'] = authorsData['Name'].apply(lambda x: x.strip())\n",
    "authorsData['Surname'] = authorsData['Surname'].apply(lambda x: x.strip().replace('-', ' '))\n",
    "\n",
    "\n",
    "authorsData['Short name'] = authorsData['Surname'] + ' ' + authorsData['Name'].str[0] + '.'\n",
    "\n",
    "\n",
    "# duplicatedAuthors = authorsData.duplicated(subset=['Name', 'Surname'])\n",
    "# print(duplicatedAuthors)\n",
    "\n",
    "print(authorsData.dtypes)\n",
    "\n",
    "# print(authorsData['Short name'])\n",
    "\n",
    "print(authorsData['Short name'].is_unique)\n",
    "\n",
    "# print(authorsData)\n",
    "\n",
    "authorsData.head(-1)\n",
    "\n",
    "authorsData.to_excel(\"data/authorsData.xlsx\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b120460",
   "metadata": {},
   "source": [
    "## 1.3 Preparation of Publication Datasets\n",
    "* Serbian Latin characters were removed  \n",
    "* A new column 'Department name' was added  \n",
    "* The following columns were removed as irrelevant: 'Art. No.', 'Page start', 'Page end', 'Page count', 'Link', 'Source', 'Volume', 'Issue'  \n",
    "* Publications were filtered by 'Document Type' (\"Article\", \"Article in Press\", \"Review\", \"Book Chapter\", \"Letter\", \"Note\")  \n",
    "* Duplicates were removed – A publication is considered a duplicate if the columns 'Authors', 'Title', 'Year', and 'Source title' are identical.\n",
    "\n",
    "### 1.3.1 Preparation of the Dataset for Epidemiology\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f6136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epidemiologyDataFiltered = epidemiologyData.copy(deep=True)\n",
    "\n",
    "# Replacing Serbian Latin specific chars with Ascii chars\n",
    "\n",
    "convertSerbianCharsToAscii(epidemiologyDataFiltered, ['Author', 'Authors', 'Title', 'Source title'])\n",
    "\n",
    "# Adding 'Department name' column\n",
    "\n",
    "epidemiologyDataFiltered['Research area'] = 'Epidemiology'\n",
    "\n",
    "# Removing unnecessary charactes from 'Author' and 'Authors' columns\n",
    "\n",
    "epidemiologyDataFiltered['Author'] = epidemiologyDataFiltered['Author'].apply(lambda x: x.strip().replace('-', ' '))\n",
    "epidemiologyDataFiltered['Authors'] = epidemiologyDataFiltered['Authors'].apply(lambda x: x.strip().replace('-', ' '))\n",
    "\n",
    "# Drop unnecessary columns\n",
    "epidemiologyDataFiltered.drop(columns=['Art. No.'], inplace=True)\n",
    "epidemiologyDataFiltered.drop(columns=['Page start'], inplace=True)\n",
    "epidemiologyDataFiltered.drop(columns=['Page end'], inplace=True)\n",
    "epidemiologyDataFiltered.drop(columns=['Page count'], inplace=True)\n",
    "epidemiologyDataFiltered.drop(columns=['Link'], inplace=True)\n",
    "epidemiologyDataFiltered.drop(columns=['Source'], inplace=True)\n",
    "epidemiologyDataFiltered.drop(columns=['Volume'], inplace=True)\n",
    "epidemiologyDataFiltered.drop(columns=['Issue'], inplace=True)\n",
    "\n",
    "# Replacing NaN Values\n",
    "\n",
    "epidemiologyDataFiltered['Cited by'] = epidemiologyDataFiltered['Cited by'].fillna(0)\n",
    "\n",
    "# Fixing types\n",
    "epidemiologyDataFiltered = epidemiologyDataFiltered.astype(\n",
    "    {\n",
    "        'Year': int, \n",
    "        'Cited by': int\n",
    "    }\n",
    ")\n",
    "print(epidemiologyDataFiltered.dtypes)\n",
    "\n",
    "# Filtering rows by 'Documnet Type'\n",
    "\n",
    "epidemiologyDataFiltered = epidemiologyDataFiltered[epidemiologyDataFiltered['Document Type'].isin(documentTypeSet)]\n",
    "\n",
    "# Checking duplcates and dropping them\n",
    "epidemiologyDataCountOld = epidemiologyDataFiltered.shape[0]\n",
    "\n",
    "epidemiologyDataFiltered = epidemiologyDataFiltered.drop_duplicates(\n",
    "    subset=duplicatesSubset,\n",
    "    keep=\"first\",\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "epidemiologyDataCountNew = epidemiologyDataFiltered.shape[0]\n",
    "\n",
    "print(f\"Number of rows before deduplication '{epidemiologyDataCountOld}'\")\n",
    "print(f\"Number of rows after deduplication '{epidemiologyDataCountNew}'\")\n",
    "print(f\"Total removed'{epidemiologyDataCountOld - epidemiologyDataCountNew}' scientific papers\")\n",
    "\n",
    "epidemiologyDataFiltered.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9244aa0",
   "metadata": {},
   "source": [
    "### 1.3.2 Preparation of Dataset for Immunology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae27f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "immunologyDataFiltered = immunologyData.copy(deep=True)\n",
    "\n",
    "# Replacing Serbian Latin specific chars with Ascii chars\n",
    "\n",
    "convertSerbianCharsToAscii(immunologyDataFiltered, ['Author', 'Authors', 'Title', 'Source title'])\n",
    "\n",
    "# Adding 'Department name' column\n",
    "\n",
    "immunologyDataFiltered['Research area'] = 'Immunology'\n",
    "\n",
    "# Removing unnecessary charactes from 'Author' and 'Authors' columns\n",
    "\n",
    "immunologyDataFiltered['Author'] = immunologyDataFiltered['Author'].apply(lambda x: x.strip().replace('-', ' '))\n",
    "immunologyDataFiltered['Authors'] = immunologyDataFiltered['Authors'].apply(lambda x: x.strip().replace('-', ' '))\n",
    "\n",
    "# Drop unnecessary columns\n",
    "immunologyDataFiltered.drop(columns=['Art. No.'], inplace=True)\n",
    "immunologyDataFiltered.drop(columns=['Page start'], inplace=True)\n",
    "immunologyDataFiltered.drop(columns=['Page end'], inplace=True)\n",
    "immunologyDataFiltered.drop(columns=['Page count'], inplace=True)\n",
    "immunologyDataFiltered.drop(columns=['Link'], inplace=True)\n",
    "immunologyDataFiltered.drop(columns=['Source'], inplace=True)\n",
    "immunologyDataFiltered.drop(columns=['Volume'], inplace=True)\n",
    "immunologyDataFiltered.drop(columns=['Issue'], inplace=True)\n",
    "\n",
    "\n",
    "# Replacing NaN Values\n",
    "\n",
    "immunologyDataFiltered['Cited by'] = immunologyDataFiltered['Cited by'].fillna(0)\n",
    "\n",
    "# Fixing types\n",
    "immunologyDataFiltered = immunologyDataFiltered.astype(\n",
    "    {\n",
    "        'Year': int, \n",
    "        'Cited by': int\n",
    "    }\n",
    ")\n",
    "print(immunologyDataFiltered.dtypes)\n",
    "\n",
    "# Filtering rows by 'Documnet Type'\n",
    "\n",
    "immunologyDataFiltered = immunologyDataFiltered[immunologyDataFiltered['Document Type'].isin(documentTypeSet)]\n",
    "\n",
    "\n",
    "# Checking duplcates and dropping them\n",
    "immunologyDataCountOld = immunologyDataFiltered.shape[0]\n",
    "\n",
    "immunologyDataFiltered = immunologyDataFiltered.drop_duplicates(\n",
    "    subset=duplicatesSubset,\n",
    "    keep=\"first\",\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "immunologyDataCountNew = immunologyDataFiltered.shape[0]\n",
    "\n",
    "print(f\"Number of rows before deduplication '{immunologyDataCountOld}'\")\n",
    "print(f\"Number of rows after deduplication '{immunologyDataCountNew}'\")\n",
    "print(f\"Total removed '{immunologyDataCountOld - immunologyDataCountNew}' scientific papaers\")\n",
    "print(f\"Title unique: {immunologyDataFiltered['Title'].is_unique}\")\n",
    "\n",
    "immunologyDataFiltered.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c08a7",
   "metadata": {},
   "source": [
    "### 1.3.3 Priprema Dataset-a za Infektivne bolesti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "infectiveDiseasesDataFiltered = infectiveDiseasesData.copy(deep=True)\n",
    "\n",
    "# Replacing Serbian Latin specific chars with Ascii chars\n",
    "convertSerbianCharsToAscii(infectiveDiseasesDataFiltered, ['Author', 'Authors', 'Title', 'Source title'])\n",
    "\n",
    "# Adding 'Department name' column\n",
    "infectiveDiseasesDataFiltered['Research area'] = 'Infective Diseases'\n",
    "\n",
    "# Removing unnecessary charactes from 'Author' and 'Authors' columns\n",
    "infectiveDiseasesDataFiltered['Author'] = infectiveDiseasesDataFiltered['Author'].apply(lambda x: x.strip().replace('-', ' '))\n",
    "infectiveDiseasesDataFiltered['Authors'] = infectiveDiseasesDataFiltered['Authors'].apply(lambda x: x.strip().replace('-', ' '))\n",
    "\n",
    "# Drop unnecessary columns\n",
    "infectiveDiseasesDataFiltered.drop(columns=['Art. No.'], inplace=True)\n",
    "infectiveDiseasesDataFiltered.drop(columns=['Page start'], inplace=True)\n",
    "infectiveDiseasesDataFiltered.drop(columns=['Page end'], inplace=True)\n",
    "infectiveDiseasesDataFiltered.drop(columns=['Page count'], inplace=True)\n",
    "infectiveDiseasesDataFiltered.drop(columns=['Link'], inplace=True)\n",
    "infectiveDiseasesDataFiltered.drop(columns=['Source'], inplace=True)\n",
    "infectiveDiseasesDataFiltered.drop(columns=['Volume'], inplace=True)\n",
    "infectiveDiseasesDataFiltered.drop(columns=['Issue'], inplace=True)\n",
    "\n",
    "\n",
    "# Replacing NaN Values\n",
    "infectiveDiseasesDataFiltered['Cited by'] = infectiveDiseasesDataFiltered['Cited by'].fillna(0)\n",
    "\n",
    "# Fixing types\n",
    "infectiveDiseasesDataFiltered = infectiveDiseasesDataFiltered.astype(\n",
    "    {\n",
    "        'Year': int, \n",
    "        'Cited by': int\n",
    "    }\n",
    ")\n",
    "print(infectiveDiseasesDataFiltered.dtypes)\n",
    "\n",
    "# Filtering rows by 'Documnet Type'\n",
    "infectiveDiseasesDataFiltered = infectiveDiseasesDataFiltered[infectiveDiseasesDataFiltered['Document Type'].isin(documentTypeSet)]\n",
    "\n",
    "\n",
    "# Checking duplcates and dropping them\n",
    "infectiveDiseasesDataCountOld = infectiveDiseasesDataFiltered.shape[0]\n",
    "\n",
    "infectiveDiseasesDataFiltered = infectiveDiseasesDataFiltered.drop_duplicates(\n",
    "    subset=duplicatesSubset,\n",
    "    keep=\"first\",\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "infectiveDiseasesDataCountNew = infectiveDiseasesDataFiltered.shape[0]\n",
    "\n",
    "print(f\"Number of rows before deduplication '{infectiveDiseasesDataCountOld}'\")\n",
    "print(f\"Number of rows after deduplication '{infectiveDiseasesDataCountNew}'\")\n",
    "print(f\"Total removed '{infectiveDiseasesDataCountOld - infectiveDiseasesDataCountNew}' scientific papaers\")\n",
    "\n",
    "infectiveDiseasesDataFiltered.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073988ba",
   "metadata": {},
   "source": [
    "### 1.3.4 Preparation of Dataset for Microbiology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbiologyDataFiltered = microbiologyData.copy(deep=True)\n",
    "\n",
    "# Replacing Serbian Latin specific chars with Ascii chars\n",
    "convertSerbianCharsToAscii(microbiologyDataFiltered, ['Author', 'Authors', 'Title', 'Source title'])\n",
    "\n",
    "# Adding 'Department name' column\n",
    "microbiologyDataFiltered['Research area'] = 'Microbiology'\n",
    "\n",
    "# Removing unnecessary charactes from 'Author' and 'Authors' columns\n",
    "microbiologyDataFiltered['Author'] = microbiologyDataFiltered['Author'].apply(lambda x: x.strip().replace('-', ' '))\n",
    "microbiologyDataFiltered['Authors'] = microbiologyDataFiltered['Authors'].apply(lambda x: x.strip().replace('-', ' '))\n",
    "\n",
    "# Drop unnecessary columns\n",
    "microbiologyDataFiltered.drop(columns=['Art. No.'], inplace=True)\n",
    "microbiologyDataFiltered.drop(columns=['Page start'], inplace=True)\n",
    "microbiologyDataFiltered.drop(columns=['Page end'], inplace=True)\n",
    "microbiologyDataFiltered.drop(columns=['Page count'], inplace=True)\n",
    "microbiologyDataFiltered.drop(columns=['Link'], inplace=True)\n",
    "microbiologyDataFiltered.drop(columns=['Source'], inplace=True)\n",
    "microbiologyDataFiltered.drop(columns=['Volume'], inplace=True)\n",
    "microbiologyDataFiltered.drop(columns=['Issue'], inplace=True)\n",
    "\n",
    "\n",
    "# Replacing NaN Values\n",
    "microbiologyDataFiltered['Cited by'] = microbiologyDataFiltered['Cited by'].fillna(0)\n",
    "\n",
    "# Fixing types\n",
    "microbiologyDataFiltered = microbiologyDataFiltered.astype(\n",
    "    {\n",
    "        'Year': int, \n",
    "        'Cited by': int\n",
    "    }\n",
    ")\n",
    "print(microbiologyDataFiltered.dtypes)\n",
    "\n",
    "# Filtering rows by 'Documnet Type'\n",
    "microbiologyDataFiltered = microbiologyDataFiltered[microbiologyDataFiltered['Document Type'].isin(documentTypeSet)]\n",
    "\n",
    "\n",
    "# Checking duplcates and dropping them\n",
    "microbiologyDataCountOld = microbiologyDataFiltered.shape[0]\n",
    "\n",
    "microbiologyDataFiltered = microbiologyDataFiltered.drop_duplicates(\n",
    "    subset=duplicatesSubset,\n",
    "    keep=\"first\",\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "microbiologyDataCountNew = microbiologyDataFiltered.shape[0]\n",
    "\n",
    "print(f\"Number of rows before deduplication '{microbiologyDataCountOld}'\")\n",
    "print(f\"Number of rows after deduplication '{microbiologyDataCountNew}'\")\n",
    "print(f\"Total removed '{microbiologyDataCountOld - microbiologyDataCountNew}' scientific papaers\")\n",
    "\n",
    "microbiologyDataFiltered.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e900214",
   "metadata": {},
   "source": [
    "## 1.4 Combining Datasets with scientific papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining filtered epidemilogoy, immunology, infectiveDiseases and microbiology dataframes\n",
    "combinedWorks = pd.concat(\n",
    "    [\n",
    "        epidemiologyDataFiltered,\n",
    "        immunologyDataFiltered,\n",
    "        infectiveDiseasesDataFiltered,\n",
    "        microbiologyDataFiltered\n",
    "        \n",
    "    ], \n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "combinedWorks = combinedWorks.drop_duplicates(\n",
    "    subset=['Authors', 'Title', 'Year', 'Source title', 'Cited by', 'Document Type'],\n",
    "    keep=\"first\",\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "combinedWorks.insert(0,'id_work',0)\n",
    "combinedWorks['id_work'] = combinedWorks.reset_index().index + 1\n",
    "combinedWorks.drop(columns=['Author'], inplace=True)\n",
    "\n",
    "print(f\"Combined dataset - Rows: {combinedWorks.shape[0]} - Cols: {combinedWorks.shape[1]}\")\n",
    "print(combinedWorks.dtypes)\n",
    "\n",
    "combinedWorks.to_excel(\"data/combinedWorks.xlsx\") \n",
    "\n",
    "combinedWorks.head(20)\n",
    "\n",
    "# combinedWorks.groupby('Source title').size().reset_index(name='Num of work per Soruce title').sort_values(by='Num of work per Soruce title', ascending=False).head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccbcfad",
   "metadata": {},
   "source": [
    "## 2. Research questions and objectives\n",
    "### 2.1 Statistical data processing\n",
    "#### 1. What is the number of works by each author? Use both whole and fractional counting. Who are the most productive scientists in the field of research and which department do they belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9fe9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation of groups of authors into separate lines\n",
    "workPerAuthor = combinedWorks.assign(Authors=combinedWorks['Authors'].str.split(',')).explode('Authors')\n",
    "workPerAuthor.rename(columns={'Authors': 'Author'}, inplace=True)\n",
    "workPerAuthor['Author'] = workPerAuthor['Author'].apply(lambda x: x.strip())\n",
    "\n",
    "def f(x, short_name):\n",
    "    if short_name in x:\n",
    "        return short_name\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "for index, row in authorsData.iterrows():\n",
    "    workPerAuthor['Author'] = workPerAuthor['Author'].apply(f, short_name = row['Short name'])\n",
    "\n",
    "workPerAuthor.to_excel(\"data/workPerAuthor.xlsx\") \n",
    "\n",
    "# Joined authorsData and workPerAuthor\n",
    "authorsAndWorks = pd.merge(authorsData, workPerAuthor, left_on='Short name', right_on='Author', how='inner')\n",
    "\n",
    "authorsAndWorks.to_excel(\"data/authorsAndWorks.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92d67c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calacualte number of works per Author by workPerAuthor DataFrame and join with authorsData DataFrame\n",
    "\n",
    "numOfWorksPerAuthor = workPerAuthor.groupby('Author').size().reset_index(name='Num of works per Author').sort_values(by='Num of works per Author', ascending=False)\n",
    "    \n",
    "numOfWorksPerAuthor.to_excel(\"data/workPerAuthorSorted.xlsx\")\n",
    "\n",
    "combineAuthorsAndNumOfWorks = pd.merge(numOfWorksPerAuthor, authorsData, left_on='Author', right_on='Short name', how='inner')\n",
    "combineAuthorsAndNumOfWorks.to_excel(\"data/firstQuestion.xlsx\")\n",
    "\n",
    "# Show the results\n",
    "# combineAuthorsAndNumOfWorks.sort_values(by='Num of works per Author', ascending=False).head(6)\n",
    "\n",
    "# Show the results\n",
    "# rezultat.head(80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba48cd8",
   "metadata": {},
   "source": [
    "#### 2. What is the average number of co-authors per author ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e1dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_work, Authors, Title, Year, Source title, Cited by, Document Type, Research area\n",
    "\n",
    "numOfAuthorsPerWork = workPerAuthor.groupby('id_work').size().reset_index(name='Num of authors per Work')\n",
    "# numOfAuthorsPerWork.head(5)\n",
    "\n",
    "numOfWorksPerAuthor = authorsAndWorks.groupby('Author').size().reset_index(name='Num of works per Author').sort_values(by='Num of works per Author', ascending=False)\n",
    "# numOfWorksPerAuthor.head(70)\n",
    "\n",
    "result1 = pd.merge(numOfAuthorsPerWork, authorsAndWorks, on='id_work', how='inner')\n",
    "# result1.head(70)\n",
    "\n",
    "result2 = pd.merge(result1, numOfWorksPerAuthor, on='Author', how='inner')\n",
    "# result.head(70)\n",
    "\n",
    "result2 = result2.groupby('Author')['Num of authors per Work'].sum().reset_index(name='Sum of Co Authors')\n",
    "# result2.head(70)\n",
    "\n",
    "result2 = pd.merge(result2, numOfWorksPerAuthor, on='Author', how='inner')\n",
    "# result2.head(70)\n",
    "\n",
    "result2['AVG CoAuthors per Author'] = (result2['Sum of Co Authors'] - result2['Num of works per Author']) / result2['Num of works per Author']\n",
    "result2.sort_values(by='Sum of Co Authors', ascending=False).head(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0b2ba5",
   "metadata": {},
   "source": [
    "#### 3. Based on the available data, determine the H-index of each scientist and compare it with the available H-index in the file authors.xlsx ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be55114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The H-index matches us to a large extent in the given excel table authors.xlsx\n",
    "authorWorksCitedBy = {}\n",
    "authorHIndex = {}\n",
    "\n",
    "for index, row in authorsData.iterrows():\n",
    "    authorWorksCitedBy[row['Short name']] = []\n",
    "\n",
    "for index, row in authorsAndWorks.iterrows():\n",
    "    authorWorksCitedBy[row['Short name']].append(row['Cited by'])\n",
    "\n",
    "for author in authorWorksCitedBy:\n",
    "    authorWorksCitedBy[author].sort(reverse=True)\n",
    "\n",
    "for author in authorWorksCitedBy:\n",
    "    HIndex = 0\n",
    "    \n",
    "    for index, item in enumerate(authorWorksCitedBy[author]):\n",
    "        if item >= index + 1:\n",
    "            HIndex = index + 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    authorHIndex[author] = HIndex\n",
    "\n",
    "authorsHIndex = pd.DataFrame(list(authorHIndex.items()), columns=['Short name', 'Calculate H index'])\n",
    "\n",
    "combinedAuthorsAndHIndex = pd.merge(authorsHIndex, authorsData, on='Short name', how='inner')\n",
    "combinedAuthorsAndHIndex.head(80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df77f1ba",
   "metadata": {},
   "source": [
    "#### 4. Which departments are the most productive in terms of scientific production and citations in journals based on the available data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a05e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "citedByPerAuthor = authorsAndWorks.groupby('Short name')['Cited by'].sum().reset_index(name='Sum of Cited by')\n",
    "# citedByPerAuthor.head(80)\n",
    "\n",
    "numOfWorksPerAuthor = authorsAndWorks.groupby('Short name').size().reset_index(name='Num of works per Author')\n",
    "# numOfWorksPerAuthor.head(80)\n",
    "\n",
    "result = pd.merge(citedByPerAuthor, numOfWorksPerAuthor, on='Short name', how='inner')\n",
    "result = pd.merge(authorsData, result, on='Short name', how='inner')\n",
    "\n",
    "result = result.groupby('Department').agg({'Sum of Cited by':'sum','Num of works per Author':'sum'})\n",
    "result.sort_values(by='Sum of Cited by', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf2476",
   "metadata": {},
   "source": [
    "#### 5. In which years were the authors most productive at the faculty level and individual departments?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dcdbd2",
   "metadata": {},
   "source": [
    "###### 5.1. By faculty level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c84d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "productivityPerYear = authorsAndWorks.groupby('Year').size().reset_index(name='Num of works')\n",
    "productivityPerYear.sort_values(by='Num of works', ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e7110a",
   "metadata": {},
   "source": [
    "###### 5.2. By department level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "productivityPerYearAndDeprtment = authorsAndWorks.groupby(['Year', 'Department']).size().reset_index(name='Num of works')\n",
    "productivityPerYearAndDeprtment.sort_values(by=['Year', 'Num of works'], ascending=[False, False]).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c34fe",
   "metadata": {},
   "source": [
    "#### 6. In which magazines is the most published on average ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cbe00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWorlsPerArticle = authorsAndWorks.groupby(['Source title']).size().reset_index(name='Num of works')\n",
    "numOfWorlsPerArticle.sort_values(by='Num of works', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc47e4",
   "metadata": {},
   "source": [
    "#### 7. Are there differences between departments in terms of volume and frequency of publication in journals ?\n",
    "From task number 6, it can be seen that the journal 'Archives of Biological Sciences' has the most publications, and in the picture below we can see that there is not an excessively large oscillation in terms of publications by journals by different departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWorksPerArticleAndDepartment = authorsAndWorks.groupby(['Source title', 'Year','Department']).size().reset_index(name='Num of works')\n",
    "# numOfWorksPerArticleAndDepartment.sort_values(by='Year', ascending=False).head(70)\n",
    "\n",
    "grafik = sns.barplot(\n",
    "    data=productivityPerYearAndDeprtment.sort_values(by=['Year'], ascending=[False]).head(36),\n",
    "    x='Year', \n",
    "    y='Num of works', \n",
    "    hue='Department'\n",
    ")\n",
    "sns.move_legend(grafik, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90531f",
   "metadata": {},
   "source": [
    "#### 8. Is there a difference in the average number of authors per paper in journals per department ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a26c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfAuthorsPerDepartment = authorsData.groupby('Department').size().reset_index(name='Num of authors')\n",
    "# numOfAuthorsPerDepartment.head(5)\n",
    "\n",
    "numOfWorksPerDepratment = combineAuthorsAndNumOfWorks.groupby('Department')['Num of works per Author'].sum().reset_index(name='Num of works')\n",
    "# numOfWorksPerDepratment.head(5)\n",
    "\n",
    "result = pd.merge(numOfAuthorsPerDepartment, numOfWorksPerDepratment, on='Department', how='inner')\n",
    "\n",
    "result['Author per work'] = result['Num of authors'] / result['Num of works']\n",
    "result['AVG Author per work'] = result['Author per work'].mean()\n",
    "result.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db8e73f",
   "metadata": {},
   "source": [
    "#### 9. What is the ratio of the number of co-authors from the faculty in relation to the number of authors outside the faculty by departments and at the level of the entire faculty ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfDomesticAuthorsPerWork = authorsAndWorks.groupby(['id_work', 'Title']).size().reset_index(name='Num of domestic authors')\n",
    "# numOfDomesticAuthorsPerWork.head(20)\n",
    "\n",
    "foreginAuthorAndWorks = pd.merge(workPerAuthor, authorsData, left_on='Author', right_on='Short name', how='left')\n",
    "filtered_df = foreginAuthorAndWorks[foreginAuthorAndWorks['Author'].isin(authorsData['Short name'])]\n",
    "foreginAuthorAndWorks = foreginAuthorAndWorks.drop(filtered_df.index)\n",
    "# foreginAuthorAndWorks.head(40)\n",
    "\n",
    "numOfForeginAuthorsPerWork = foreginAuthorAndWorks.groupby(['id_work', 'Title']).size().reset_index(name='Num of foregin authors')\n",
    "# numOfForeginAuthorsPerWork.head(40)\n",
    "\n",
    "numOfForeginAndDomesitAuthorsPerWork = pd.merge(numOfDomesticAuthorsPerWork, numOfForeginAuthorsPerWork, on=['id_work', 'Title'])\n",
    "# numOfForeginAndDomesitAuthorsPerWork.head(40)\n",
    "\n",
    "mergedDomesticAuhorsWorksAndNumOfCoAuthors = pd.merge(authorsAndWorks, numOfForeginAndDomesitAuthorsPerWork, on='id_work', how='inner')\n",
    "# result9.head(25)\n",
    "\n",
    "noDuplicatedWorks = mergedDomesticAuhorsWorksAndNumOfCoAuthors[~mergedDomesticAuhorsWorksAndNumOfCoAuthors.duplicated(['id_work', 'Department'], keep=False)]\n",
    "# df_no_duplicates.head(10)\n",
    "\n",
    "groupbyDepartment = noDuplicatedWorks.groupby('Department')[['Num of domestic authors', 'Num of foregin authors']].sum()\n",
    "groupbyDepartment['The ratio of domestic and foregin authors'] = groupbyDepartment['Num of domestic authors'] / groupbyDepartment['Num of foregin authors']\n",
    "\n",
    "groupbyDepartment.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1293149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "foreginAuthorAndWorks.groupby('Author').size().reset_index(name='Num of works per Author').sort_values(by='Num of works per Author', ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9306a76",
   "metadata": {},
   "source": [
    "### 2.2 Basic characterization of modeled networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Export DataFrame 'authorsAndWorks' via Pickle to a file and load it here\n",
    "inputCleanData = authorsAndWorks\n",
    "labels = set(inputCleanData['Short name'].unique())\n",
    "print(f\"There are { len(labels) } different authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a727d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedAuthorsAndWorks = inputCleanData[['id_work', 'Author']]\n",
    "\n",
    "combinations_df = pd.merge(reducedAuthorsAndWorks, reducedAuthorsAndWorks, on='id_work')\n",
    "# combinations_df.head(80)\n",
    "\n",
    "# Filtering pairs of authors that are not the same (to avoid pairs of authors with themselves)\n",
    "filtered_combinations_df = combinations_df[combinations_df['Author_x'] != combinations_df['Author_y']]\n",
    "\n",
    "# Creating a new DataFrame with columns 'id_work', 'author' and 'co-author'\n",
    "result_df = filtered_combinations_df[['id_work', 'Author_x', 'Author_y']].rename(columns={'Author_x': 'Author', 'Author_y': 'Co-Author'})\n",
    "result_df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d128e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(labels)\n",
    "\n",
    "for _, idWork, author, coAuthor in result_df.itertuples():\n",
    "    if (author, coAuthor) in G.edges:\n",
    "        G.edges[author, coAuthor]['weight'] += 1\n",
    "    else:\n",
    "        G.add_edge(author, coAuthor, weight=1)\n",
    "\n",
    "print(G.edges.data(\"weight\"))\n",
    "\n",
    "nx.write_gml(G, \"models/mreza_jedan.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb119c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = G.to_undirected()\n",
    "\n",
    "print(\"Number of connected components: \" , nx.number_connected_components(H))\n",
    "\n",
    "connected_componnets = sorted(nx.connected_components(H), key=len, reverse=True)\n",
    "print(\"Size of connected components:\")\n",
    "for x in connected_componnets:\n",
    "    print(len(x) , \"  \")\n",
    "    \n",
    "print( \"Average clustering coefficient:\" , nx.average_clustering(H), \", Global clustering coefficient: \" , nx.transitivity(H) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Degree of clustering\n",
    "author, clustering_coef = zip(*nx.clustering(G, weight = \"weight\").items())\n",
    "no_zero = [(auth, cc)  for auth, cc in zip(author, clustering_coef) if cc > 0]\n",
    "\n",
    "df = pd.DataFrame(no_zero, columns = [\"Author\", \"Cc\"])\n",
    "df.sort_values('Cc', inplace = True)\n",
    "\n",
    "max_local_degree_of_clustering = max(clustering_coef)\n",
    "average_degree_of_clustering = nx.average_clustering(G)\n",
    "\n",
    "print(f\"Max local cc: {max_local_degree_of_clustering}\")\n",
    "print(f\"Average cc: {average_degree_of_clustering}\")\n",
    "print(\"Local degree of clustering which is not a zero:\")\n",
    "print(df)\n",
    "\n",
    "# ax = df.plot.scatter(x='Author', y='Cc')\n",
    "# ax.set_xscale(\"log\")\n",
    "# ax.set_yscale(\"log\");\n",
    "\n",
    "plt.hist(clustering_coef, bins=10, edgecolor='black', color = \"green\")\n",
    "\n",
    "# Set axis titles and labels\n",
    "plt.title('Local Clustering Coefficient Histogram')\n",
    "plt.xlabel('Local Clustering Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('images/local_cc_main_network.png')\n",
    "\n",
    "# Displaying a histogram\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea545c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deg_frequency(G, weighted = False, xscale = \"log\", yscale = \"log\"):\n",
    "\n",
    "    if weighted:\n",
    "        degrees = G.degree(weight=\"weight\")\n",
    "    else:\n",
    "        degrees = G.degree()\n",
    "        \n",
    "    _, deg_list = zip(*degrees)\n",
    "    deg_counts = Counter(deg_list)        \n",
    "    print(deg_counts)\n",
    "    x, y = zip(*deg_counts.items())                                                      \n",
    "\n",
    "    plt.figure(1)   \n",
    "\n",
    "    # prep axes   \n",
    "    if weighted:\n",
    "        plt.xlabel('weighted degree')  \n",
    "    else:\n",
    "        plt.xlabel('degree')                                                                                                             \n",
    "    plt.xscale(xscale)                                                                                                                \n",
    "    plt.xlim(1, max(x))  \n",
    "\n",
    "    plt.ylabel('frequency')                                                                                                          \n",
    "    plt.yscale(yscale)                                                                                                                \n",
    "    plt.ylim(1, max(y))                                                                                                             \n",
    "                                                                                                                                                                                                    \n",
    "    plt.scatter(x, y, marker='.')                                                                                                    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd816cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "\n",
    "print(f\"Nodes: {n}\")\n",
    "print(f\"Edges: {m}\")\n",
    "\n",
    "Gnm = nx.gnm_random_graph(n, m) \n",
    "\n",
    "# plot_deg_frequency(Gnm, xscale = 'linear', yscale = 'linear')\n",
    "plot_deg_frequency(Gnm, xscale = 'log', yscale = 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a080983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erdős–Rényi network N = 58 (number of nodes - authors)\n",
    "\n",
    "p = (2 * float(m)) / (n * ( n - 1))\n",
    "print(p)\n",
    "\n",
    "er_network = nx.erdos_renyi_graph(n,p)\n",
    "\n",
    "delta_m = m - er_network.number_of_edges()\n",
    "print(f\"The number of nodes of the original network minus the number of nodes in the ER network amounts to {delta_m}, what is the deviation from {abs(float(delta_m)) * 100 / m}%\")\n",
    "\n",
    "# plot_deg_frequency(er_network, xscale = 'linear', yscale = 'linear')\n",
    "plot_deg_frequency(er_network, xscale = 'log', yscale = 'log')\n",
    "\n",
    "# Local clustering degree\n",
    "author, clustering_coef = zip(*nx.clustering(er_network, weight = \"weight\").items())\n",
    "no_zero = [(auth, cc)  for auth, cc in zip(author, clustering_coef) if cc > 0]\n",
    "\n",
    "df = pd.DataFrame(no_zero, columns = [\"Author\", \"Cc\"])\n",
    "df.sort_values('Cc', inplace = True)\n",
    "\n",
    "max_local_degree_of_clustering = max(clustering_coef)\n",
    "prosecni_stepen_klasterisanja = nx.average_clustering(er_network)\n",
    "\n",
    "print(f\"Max local cc Erods-Reny network: {max_local_degree_of_clustering}\")\n",
    "print(f\"Average cc Erods-Reny network: {prosecni_stepen_klasterisanja}\")\n",
    "print(\"Local clustering degree which isn't zero Erods-Reny network:\")\n",
    "print(df)\n",
    "\n",
    "plt.hist(clustering_coef, bins=10, edgecolor='black', color = \"red\")\n",
    "\n",
    "# Axis title and label settings\n",
    "plt.title('Local Clustering Coefficient Histogram')\n",
    "plt.xlabel('Local Clustering Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('images/local_cc_er_network.png')\n",
    "\n",
    "# Show histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random generated ScaleFree network\n",
    "scale_free_network = nx.scale_free_graph(n)\n",
    "\n",
    "# Create weighted graph from scale_free_network\n",
    "Q = nx.DiGraph()\n",
    "for u,v in scale_free_network.edges():\n",
    "    if Q.has_edge(u,v):\n",
    "        Q[u][v]['weight'] += 1\n",
    "    else:\n",
    "        Q.add_edge(u, v, weight=1)\n",
    "        \n",
    "# Local clustering degree\n",
    "author, clustering_coef = zip(*nx.clustering(Q, weight = \"weight\").items())\n",
    "nenula = [(auth, cc)  for auth, cc in zip(author, clustering_coef) if cc > 0]\n",
    "\n",
    "df = pd.DataFrame(nenula, columns = [\"Author\", \"Cc\"])\n",
    "df.sort_values('Cc', inplace = True)\n",
    "\n",
    "max_local_degree_of_clustering = max(clustering_coef)\n",
    "\n",
    "average_degree_of_clustering = nx.average_clustering(Q)\n",
    "\n",
    "print(f\"Max local cc Random scale-free network: {max_local_degree_of_clustering}\")\n",
    "print(f\"Average cc Random scale-free network: {average_degree_of_clustering}\")\n",
    "print(\"Local clustering degree which isn't zero Random scale-free network:\")\n",
    "print(df)\n",
    "\n",
    "plt.hist(clustering_coef, bins=10, edgecolor='black')\n",
    "\n",
    "plt.title('Local Clustering Coefficient Histogram')\n",
    "plt.xlabel('Local Clustering Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.savefig('images/local_cc_random_scale_free_network.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assortativeness \n",
    "# Assortativeness based on the unweighted degree of the node\n",
    "r1 = nx.degree_assortativity_coefficient(G)\n",
    "print(f\"Coefficient of assortativity based on the unweighted degree of the node: {r1}\")\n",
    "\n",
    "# Assortability based on the degree of difficulty of the node\n",
    "r2 = nx.degree_assortativity_coefficient(G, weight='weight')\n",
    "print(f\"Coefficient of assortativity based on the weight degree of the node: {r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a06947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rich club phenomenon; Havel-Hakimi algorithm\n",
    "degrees = G.degree()\n",
    "_, deg_list = zip(*degrees)\n",
    "print(deg_list)\n",
    "\n",
    "havel_hakimi_network = nx.havel_hakimi_graph(deg_list)\n",
    "\n",
    "# nx.draw(havel_hakimi_network)\n",
    "nx.write_gml(havel_hakimi_network, \"models/havel_hakimi.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_club_coeffs = nx.rich_club_coefficient(G.to_undirected())\n",
    "rich_club_coeffs_havel_hakimi = nx.rich_club_coefficient(havel_hakimi_network)\n",
    "\n",
    "if rich_club_coeffs:\n",
    "    degrees = list(rich_club_coeffs.keys())\n",
    "    coefficients = list(rich_club_coeffs.values())\n",
    "    print(degrees)\n",
    "    print(coefficients)\n",
    "    plt.plot(degrees, coefficients, marker='o')\n",
    "    plt.xlabel('Degree (k)')\n",
    "    plt.ylabel('Rich-club Coefficient')\n",
    "    plt.title('Rich-club Phenomenon')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The network does not contain enough nodes to analyze the rich-club phenomenon.\")\n",
    "\n",
    "if rich_club_coeffs_havel_hakimi:\n",
    "    degrees = list(rich_club_coeffs_havel_hakimi.keys())\n",
    "    coefficients = list(rich_club_coeffs_havel_hakimi.values())\n",
    "    print(degrees)\n",
    "    print(coefficients)\n",
    "    plt.plot(degrees, coefficients, marker='o', color='red')\n",
    "    plt.xlabel('Degree (k)')\n",
    "    plt.ylabel('Rich-club Coefficient')\n",
    "    plt.title('Rich-club Phenomenon Havel-Hakimi')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The network does not contain enough nodes to analyze the rich-club phenomenon.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power law distribution\n",
    "degrees = dict(G.degree())\n",
    "degree_values = list(degrees.values())\n",
    "\n",
    "plt.hist(degree_values, bins=30, log=True)\n",
    "plt.title('Degree Distribution Histogram')\n",
    "plt.xlabel('Number of nodes')\n",
    "plt.ylabel('Number of nodes (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67938042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assortative mixing\n",
    "def plot_deg_corr(g, xscale = \"linear\", yscale = \"linear\"):\n",
    "    first = []\n",
    "    second = []\n",
    "    for i, k in g.edges():\n",
    "        first.append(g.degree(i))\n",
    "        first.append(g.degree(k))\n",
    "        second.append(g.degree(k))\n",
    "        second.append(g.degree(i))\n",
    "\n",
    "    plt.figure(1)   \n",
    "\n",
    "    plt.xlabel('degree')                                                                                                             \n",
    "    plt.xscale(xscale)                                                                                                                \n",
    "    plt.xlim(1, max(first)) \n",
    "\n",
    "    plt.ylabel('degree')                                                                                                          \n",
    "    plt.yscale(yscale)                                                                                                                \n",
    "    plt.ylim(1, max(second))                                                                                                             \n",
    "\n",
    "    plt.scatter(first, second, marker='.')                                                                                                    \n",
    "    plt.show()\n",
    "\n",
    "plot_deg_corr(G)\n",
    "\n",
    "assortativity = nx.degree_assortativity_coefficient(G)\n",
    "print(f\"Mixing assortativeness coefficient: {assortativity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0904872",
   "metadata": {},
   "source": [
    "### 2.3 Analysis of centrality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58685b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_centralities(G):\n",
    "\n",
    "    DC_dict = nx.degree_centrality(G)\n",
    "    CC_dict = nx.closeness_centrality(G)\n",
    "    BC_dict = nx.betweenness_centrality(G)\n",
    "    EVC_dict = nx.eigenvector_centrality(G)\n",
    "\n",
    "    df1 = pd.DataFrame.from_dict(DC_dict, orient='index', columns=['DC'])\n",
    "    df2 = pd.DataFrame.from_dict(CC_dict, orient='index', columns=['CC'])\n",
    "    df3 = pd.DataFrame.from_dict(BC_dict, orient='index', columns=['BC'])\n",
    "    df4 = pd.DataFrame.from_dict(EVC_dict, orient='index', columns=['EVC'])\n",
    "    df = pd.concat([df1, df2, df3, df4], axis=1)\n",
    "    return df\n",
    "\n",
    "calcCentralitiesDf = calculate_centralities(G)\n",
    "\n",
    "print(calcCentralitiesDf)\n",
    "print(calcCentralitiesDf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_centralities_v2(G):\n",
    "\n",
    "    DC_dict = nx.degree_centrality(G)\n",
    "    CC_dict = nx.closeness_centrality(G)\n",
    "    BC_dict = nx.betweenness_centrality(G)\n",
    "    EVC_dict = nx.eigenvector_centrality(G)\n",
    "        \n",
    "    df1 = pd.DataFrame(list(DC_dict.items()), columns=['Author', 'Degree centrality'])\n",
    "    df2 = pd.DataFrame(list(CC_dict.items()), columns=['Author', 'Closeness centrality'])\n",
    "    df3 = pd.DataFrame(list(BC_dict.items()), columns=['Author', 'Betweenness centrality'])\n",
    "    df4 = pd.DataFrame(list(EVC_dict.items()), columns=['Author', 'Eigenvector centrality'])\n",
    "    df = df1.merge(df2,on='Author').merge(df3,on='Author').merge(df4,on='Author')\n",
    "    return df\n",
    "\n",
    "\n",
    "centralitiesDf = calculate_centralities_v2(G)\n",
    "\n",
    "centralitiesDf = centralitiesDf.merge(authorsData, left_on='Author', right_on='Kratko ime', how='inner')\n",
    "centralitiesDf.sort_values(by='Degree centrality', ascending=False).head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "centralitiesDf.sort_values(by='Eigenvector centrality', ascending=False).head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristics\n",
    "labels = ['DC', 'CC', 'BC', 'EVC']\n",
    "cross_correlation_matrix = pd.DataFrame(columns = ['DC', 'CC', 'BC', 'EVC'], index = ['DC', 'CC', 'BC', 'EVC'])\n",
    "p_val_matrix = pd.DataFrame(columns = ['DC', 'CC', 'BC', 'EVC'], index = ['DC', 'CC', 'BC', 'EVC'])\n",
    "\n",
    "for ind in labels:\n",
    "    for col in labels:\n",
    "        cross_correlation_matrix[ind][col], p_val_matrix[ind][col] = stats.kendalltau(calcCentralitiesDf[ind], calcCentralitiesDf[col])\n",
    "\n",
    "print(cross_correlation_matrix)\n",
    "print(p_val_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = calculate_centralities(G)\n",
    "for metric in labels:\n",
    "    df_[f\"{metric}_rank\"] = df_[f\"{metric}\"].rank(ascending=False) \n",
    "    \n",
    "df_['composite_rank'] = df_['DC_rank'] * df_['CC_rank'] * df_['BC_rank'] * df_['EVC_rank']\n",
    "\n",
    "df_.sort_values(['composite_rank'], ascending = True)\n",
    "\n",
    "#  df_.sort_values(['CC'], ascending = False, inplace=True)\n",
    "\n",
    "df_.head(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcbd50",
   "metadata": {},
   "source": [
    "## 3. Commune detection by spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3774bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grivan-Newman Method\n",
    "\n",
    "# grivanNewman = nx.community.girvan_newman(G)\n",
    "# print(grivanNewman)\n",
    "\n",
    "# k = 10\n",
    "# limited = itertools.takewhile(lambda c: len(c) <= k, grivanNewman)\n",
    "# for communities in limited:\n",
    "# print(tuple(sorted(c) for c in communities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027915c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grivanNewman = nx.community.girvan_newman(G)\n",
    "\n",
    "k = 6  # set the desired number of communes\n",
    "communities = list(sorted(c) for c in next(grivanNewman) if len(c) >= k)\n",
    "\n",
    "# G = nx.path_graph(10)\n",
    "communities = list(nx.community.girvan_newman(G))\n",
    "\n",
    "# building initial dict of node_id to each possible subset:\n",
    "node_id = 0\n",
    "init_node2community_dict = {node_id: communities[0][0].union(communities[0][1])}\n",
    "for comm in communities:\n",
    "    for subset in list(comm):\n",
    "        if subset not in init_node2community_dict.values():\n",
    "            node_id += 1\n",
    "            init_node2community_dict[node_id] = subset\n",
    "\n",
    "# turning this dictionary to the desired format in @mdml's answer\n",
    "node_id_to_children = {e: [] for e in init_node2community_dict.keys()}\n",
    "for node_id1, node_id2 in combinations(init_node2community_dict.keys(), 2):\n",
    "    for node_id_parent, group in init_node2community_dict.items():\n",
    "        if len(init_node2community_dict[node_id1].intersection(init_node2community_dict[node_id2])) == 0 and group == init_node2community_dict[node_id1].union(init_node2community_dict[node_id2]):\n",
    "            node_id_to_children[node_id_parent].append(node_id1)\n",
    "            node_id_to_children[node_id_parent].append(node_id2)\n",
    "\n",
    "# also recording node_labels dict for the correct label for dendrogram leaves\n",
    "node_labels = dict()\n",
    "for node_id, group in init_node2community_dict.items():\n",
    "    if len(group) == 1:\n",
    "        node_labels[node_id] = list(group)[0]\n",
    "    else:\n",
    "        node_labels[node_id] = ''\n",
    "\n",
    "# also needing a subset to rank dict to later know within all k-length merges which came first\n",
    "subset_rank_dict = dict()\n",
    "rank = 0\n",
    "for e in communities[::-1]:\n",
    "    for p in list(e):\n",
    "        if tuple(p) not in subset_rank_dict:\n",
    "            subset_rank_dict[tuple(sorted(p))] = rank\n",
    "            rank += 1\n",
    "subset_rank_dict[tuple(sorted(chain.from_iterable(communities[-1])))] = rank\n",
    "\n",
    "# function to get a merge height so that it is unique (probably not that efficient)\n",
    "def get_merge_height(sub):\n",
    "    sub_tuple = tuple(sorted([node_labels[i] for i in sub]))\n",
    "    n = len(sub_tuple)\n",
    "    other_same_len_merges = {k: v for k, v in subset_rank_dict.items() if len(k) == n}\n",
    "    min_rank, max_rank = min(other_same_len_merges.values()), max(other_same_len_merges.values())\n",
    "    range = (max_rank-min_rank) if max_rank > min_rank else 1\n",
    "    return float(len(sub)) + 0.8 * (subset_rank_dict[sub_tuple] - min_rank) / range\n",
    "\n",
    "# finally using @mdml's magic, slightly modified:\n",
    "G           = nx.DiGraph(node_id_to_children)\n",
    "nodes       = G.nodes()\n",
    "leaves      = set( n for n in nodes if G.out_degree(n) == 0 )\n",
    "inner_nodes = [ n for n in nodes if G.out_degree(n) > 0 ]\n",
    "\n",
    "# Compute the size of each subtree\n",
    "subtree = dict( (n, [n]) for n in leaves )\n",
    "for u in inner_nodes:\n",
    "    children = set()\n",
    "    node_list = list(node_id_to_children[u])\n",
    "    while len(node_list) > 0:\n",
    "        v = node_list.pop(0)\n",
    "        children.add( v )\n",
    "        node_list += node_id_to_children[v]\n",
    "    subtree[u] = sorted(children & leaves)\n",
    "\n",
    "inner_nodes.sort(key=lambda n: len(subtree[n])) # <-- order inner nodes ascending by subtree size, root is last\n",
    "\n",
    "# Construct the linkage matrix\n",
    "leaves = sorted(leaves)\n",
    "index  = dict( (tuple([n]), i) for i, n in enumerate(leaves) )\n",
    "Z = []\n",
    "k = len(leaves)\n",
    "for i, n in enumerate(inner_nodes):\n",
    "    children = node_id_to_children[n]\n",
    "    x = children[0]\n",
    "    for y in children[1:]:\n",
    "        z = tuple(sorted(subtree[x] + subtree[y]))\n",
    "        i, j = index[tuple(sorted(subtree[x]))], index[tuple(sorted(subtree[y]))]\n",
    "        Z.append([i, j, get_merge_height(subtree[n]), len(z)]) # <-- float is required by the dendrogram function\n",
    "        index[z] = k\n",
    "        subtree[z] = list(z)\n",
    "        x = z\n",
    "        k += 1\n",
    "\n",
    "# Dendrogram\n",
    "plt.figure()\n",
    "dendrogram(Z, labels=[node_labels[node_id] for node_id in leaves])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11339646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In gephi, we put 2.5 for Community Detection, for a case similar to a dendogram\n",
    "# Importing the required library\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Defining the number of clusters\n",
    "n_clusters = 3\n",
    "\n",
    "# Calculation of spectral clustering\n",
    "clustering = SpectralClustering(n_clusters=n_clusters, assign_labels=\"discretize\", affinity=\"precomputed\").fit(nx.adjacency_matrix(G).toarray())\n",
    "\n",
    "colors = clustering.labels_\n",
    "c_string = []\n",
    "for c in colors:\n",
    "    c_string.append(str(c))\n",
    "\n",
    "spectral_clust_graph = nx.Graph()\n",
    "for c, label in zip(c_string, G.nodes()):\n",
    "    spectral_clust_graph.add_node(label, color=c)\n",
    "\n",
    "for edge in G.edges(data=True):\n",
    "    #print(edge)\n",
    "    spectral_clust_graph.add_edge(edge[0], edge[1], weight=edge[2]['weight'])\n",
    "\n",
    "nx.write_pajek(spectral_clust_graph, \"models/spectral4.net\")\n",
    "\n",
    "csizes = np.zeros(n_clusters)\n",
    "for c in colors:\n",
    "    csizes[int(c)] += 1\n",
    "\n",
    "print(f\"Division into {n_clusters}: the component sizes are {csizes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
